{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (env MLFlow_new)",
      "language": "python",
      "name": "py-dku-venv-mlflow_new"
    },
    "associatedRecipe": "recipe_from_notebook_Custom_ModelingLogistic_Regression",
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "admin"
      },
      "lastModifiedOn": 1699810205065,
      "extendedProperties": {}
    },
    "creator": "admin",
    "createdOn": 1699810205065,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import warnings\n",
        "\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
        "from dataikuapi.dss.ml import DSSPredictionMLTaskSettings\n",
        "warnings.filterwarnings(\u0027ignore\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Replace these constants by your own values\n",
        "EXPERIMENT_TRACKING_FOLDER_NAME \u003d \"PD_tracking\"\n",
        "EXPERIMENT_TRACKING_FOLDER_CONNECTION \u003d \"filesystem_folders\"\n",
        "EXPERIMENT_NAME \u003d \"pd-model\"\n",
        "\n",
        "MLFLOW_CODE_ENV_NAME \u003d \"Mlflow_new\"\n",
        "SAVED_MODEL_NAME \u003d \"ProbDefault-model\"\n",
        "DATASET_TRAINING \u003d \"pd_train\""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Some utils\n",
        "def now_str() -\u003e str:\n",
        "    return datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment tracking (scikit-learn)\n",
        "\n",
        "This notebook contains a simple example to showcase the new Experiment Tracking capabilities of Dataiku. It explains how to perform several runs with different parameters, select the best run and promote it as a Saved Model version in a Dataiku Flow. It leverages:\n",
        "* the scikit-learn package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the training data\n",
        "\n",
        "Our training data lives in the `labeled` Dataset, let\u0027s load it in a pandas DataFrame and see what it looks like:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "client \u003d dataiku.api_client()\n",
        "project \u003d client.get_default_project()\n",
        "training_dataset \u003d dataiku.Dataset(DATASET_TRAINING)\n",
        "df \u003d training_dataset.get_dataframe()\n",
        "df.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are working on a *binary classification* problem here, which is to predict whether or not a given customer is high value. This outcome is reflected by the `Default` column which can either take the \"0.0\" or \"1.0\" values."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "target_name \u003d \"Default\"\n",
        "target \u003d df[target_name]\n",
        "data \u003d df.drop(columns\u003d[target_name])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get-or-create Managed Folder (WIP)\n",
        "project_folders \u003d project.list_managed_folders()\n",
        "folder \u003d None\n",
        "if len(project_folders) \u003e 0:\n",
        "    for mf in project_folders:\n",
        "        if mf[\"name\"] \u003d\u003d EXPERIMENT_TRACKING_FOLDER_NAME:\n",
        "            folder_id \u003d mf[\"id\"]\n",
        "            print(f\"Found experiment tracking folder {EXPERIMENT_TRACKING_FOLDER_NAME} with id {mf[\u0027id\u0027]}\")\n",
        "            folder \u003d project.get_managed_folder(odb_id\u003dfolder_id)\n",
        "            break\n",
        "        else:\n",
        "            continue\n",
        "    # -- If you reach this point, you didn\u0027t find the experiment tracking folder among the existing ones.\n",
        "    if not folder:\n",
        "        print(\"Experiment tracking folder not found. Creating it...\")\n",
        "        folder \u003d project.create_managed_folder(EXPERIMENT_TRACKING_FOLDER_NAME,\n",
        "                                   connection_name\u003dEXPERIMENT_TRACKING_FOLDER_CONNECTION)\n",
        "else:\n",
        "    print(\"No folder found in project. Creating one for experiment tracking...\")\n",
        "    # Write the creation of the mf code here.\n",
        "    folder \u003d project.create_managed_folder(EXPERIMENT_TRACKING_FOLDER_NAME,\n",
        "                                       connection_name\u003dEXPERIMENT_TRACKING_FOLDER_CONNECTION)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing the experiment\n",
        "\n",
        "To prepare the grounds for our experiments, we need to create a few handles and define which MLFlow experiment we\u0027ll collect our runs into:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a mlflow_extension object to easily collect information for the promotion step\n",
        "mlflow_extension \u003d project.get_mlflow_extension()\n",
        "\n",
        "# Create a handle for the mlflow client\n",
        "mlflow_handle \u003d project.setup_mlflow(managed_folder\u003dfolder)\n",
        "\n",
        "# Set the experiment\n",
        "experiment \u003d mlflow.set_experiment(experiment_name\u003dEXPERIMENT_NAME)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experimenting\n",
        "\n",
        "The goal of experiment tracking is to *instrument the iterative process of ML model training* by collecting all parameters and results of each trial. To be more specific, within an **experiment**, you perform multiple **runs**, each run being different from the others because of the **parameters** you use for it. You also need to specific which **metrics** to track, they will reflect the performance of the model for a given set of parameters.\n",
        "\n",
        "In this notebook example, if you want to produce experiment runs:\n",
        "* edit the parameters in the 3.1 cell and run it\n",
        "* run the 3.2 cell to effectively... perform the run ðŸ™‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the parameters of our run"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create run name\n",
        "run_params \u003d {}\n",
        "run_metrics \u003d {}\n",
        "\n",
        "# Define run parameters\n",
        "# -- Which columns to retain ?\n",
        "categorical_cols \u003d [\"State\", \"Loan_Purpose\", \"Home_Ownership\"]\n",
        "run_params[\"categorical_cols\"] \u003d categorical_cols\n",
        "numerical_cols \u003d [\"Monthly_Income\",\"Interest_Rate\", \"FICO_avg\", \"Amount_Requested\", \"Loan_Length\"]\n",
        "run_params[\"numerical_cols\"] \u003d numerical_cols\n",
        "\n",
        "# --Which algorithm to use? Which hyperparameters for this algo to try?\n",
        "# --- Example: Random Forest\n",
        "hparams \u003d {\"penalty\":\"l2\",\n",
        "           \"class_weight\": \"balanced\",\n",
        "           \"solver\":\"lbfgs\",\n",
        "           \"max_iter\":100}\n",
        "\n",
        "clf \u003d LogisticRegression(**hparams)\n",
        "model_algo \u003d type(clf).__name__\n",
        "run_params[\"model_algo\"] \u003d model_algo\n",
        "for hp in hparams.keys():\n",
        "    run_params[hp] \u003d hparams[hp]\n",
        "\n",
        "# --Which cross-validation settings to use?\n",
        "n_cv_folds \u003d 5\n",
        "cv \u003d StratifiedKFold(n_splits\u003dn_cv_folds)\n",
        "run_params[\"n_cv_folds\"] \u003d n_cv_folds\n",
        "metrics \u003d [\"f1_macro\", \"roc_auc\"]\n",
        "\n",
        "# --Let\u0027s print all of that to get a recap:\n",
        "print(f\"Parameters to log:\\n {run_params}\")\n",
        "print(100*\u0027-\u0027)\n",
        "print(f\"Metrics to log:\\n {metrics}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performing the run and logging parameters, metrics and the model"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "run_ts \u003d now_str()\n",
        "run_name \u003d f\"run-{run_ts}\"\n",
        "with mlflow.start_run(run_name\u003drun_name) as run:\n",
        "    run_id \u003d run.info.run_id\n",
        "    print(f\"Starting run {run_name} (id: {run_id})...\")\n",
        "    # --Preprocessing\n",
        "    categorical_preprocessor \u003d OneHotEncoder(handle_unknown\u003d\"ignore\")\n",
        "    preprocessor \u003d ColumnTransformer([\n",
        "        (\u0027categorical\u0027, categorical_preprocessor, categorical_cols),\n",
        "        (\u0027numerical\u0027, \u0027passthrough\u0027, numerical_cols)])\n",
        "\n",
        "    # --Pipeline definition (preprocessing + model)\n",
        "    pipeline \u003d make_pipeline(preprocessor, clf)\n",
        "\n",
        "    # --Cross-validation\n",
        "    print(f\"Running cross-validation...\")\n",
        "    scores \u003d cross_validate(pipeline, data, target, cv\u003dcv, scoring\u003dmetrics)\n",
        "    for m in [f\"test_{mname}\" for mname in metrics]:\n",
        "        run_metrics[f\"mean_{m}\"] \u003d scores[m].mean()\n",
        "        run_metrics[f\"std_{m}\"] \u003d scores[m].std()\n",
        "\n",
        "    # --Pipeline fit\n",
        "    pipeline.fit(X\u003ddata, y\u003dtarget)\n",
        "    # --Log the order of the class label\n",
        "    run_params[\"class_labels\"] \u003d [str(c) for c in pipeline.classes_.tolist()]\n",
        "\n",
        "    # --Log parameters, metrics and model\n",
        "    mlflow.log_params(params\u003drun_params)\n",
        "    mlflow.log_metrics(metrics\u003drun_metrics)\n",
        "    artifact_path \u003d f\"{model_algo}-{run_id}\"\n",
        "    mlflow.sklearn.log_model(sk_model\u003dpipeline, artifact_path\u003dartifact_path)\n",
        "\n",
        "    # --Set useful information to faciliate run promotion\n",
        "    mlflow_extension.set_run_inference_info(run_id\u003drun_id,\n",
        "                                            prediction_type\u003d\"BINARY_CLASSIFICATION\",\n",
        "                                            classes\u003drun_params[\"class_labels\"],\n",
        "                                            code_env_name\u003dMLFLOW_CODE_ENV_NAME,\n",
        "                                            target\u003d\"Default\")\n",
        "    print(f\"DONE! Your artifacts are available at {run.info.artifact_uri}\")"
      ],
      "outputs": []
    }
  ]
}